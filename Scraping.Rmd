---
title: "2018 SICSS"
author: "Myeong Lee"
date: "6/19/2018"
output: html_document
---

# Web Scraping

```{r setup, include=FALSE}
library (rvest)
library(Rselenium)
library(ggplot2)
```


# using XPath
```{r}
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")

section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table')
head(section_of_wikipedia)
health_rankings<-html_table(section_of_wikipedia)

```


## using SelectorGadget
```{r}
duke_page<-read_html("https://www.duke.edu")
duke_events<-html_nodes(duke_page, css="li:nth-child(1) .epsilon")
raw_text <- html_text(duke_events)


```


# RSelenium for Browser Automation
```{r}

```


# Facebook Scraping
```{r}

library(Rfacebook)

token <- "EAACEdEose0cBAN89FQTxAeXROcCI4ZBbnit2rZBzxb0NZApzccfJrFKCIMw1eQGz6jtsf3ew2VRelWvXEe5e12idoSq2qaAILJN3OIZApeu7LLJSdiXKVO3qvdfAFhg1ReJSHUjOvAOpeZCfdw3DULZAOPdUvaBuXjUZC1Vo7ZC6Co8JJ1LKx7AWyOEfCIb8jiAZD"

# getUsers("me", token=token, private_info = FALSE)
# getFQL("SELECT eid FROM event_member WHERE uid IN (SELECT page_id FROM place WHERE distance(latitude, longitude, 37.76, -122.427) < 200)", token=token)
# getEvents("161413941074998", token=token, api="v3.0")


nyt <- getPage("156609491019905", token=token, n=1000)
ndwa <- getPage("357750373755", token=token, n=1000)


```



# Corpus
```{r}
library(tm)
library(tidytext)
library(dplyr)
library(SnowballC)

load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text))) 

tidy_trump_tweets<- trumptweets %>% select(created_at,text) %>% unnest_tokens("word", text)
tidy_trump_tweets %>% count(word) %>% arrange(desc(n))

data("stop_words")
tidy_trump_tweets <- tidy_trump_tweets %>% anti_join(stop_words)

#removing numbers
tidy_trump_tweets<-tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]

# removing whitespaces
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)

# Stemming
tidy_trump_tweets <- tidy_trump_tweets %>% mutate_at("word", funs(wordStem((.), language="en")))

# Document Term Matrix
tidy_trump_DTM <- tidy_trump_tweets %>% count(created_at, word) %>% cast_dtm(created_at, word, n)

# TF-IDF ("document_col" needs to be in number or character)
tidy_trump_tweets$created_at <- as.character(tidy_trump_tweets$created_at)
tidy_trump_tfidf<- tidy_trump_tweets%>% count(word, created_at) %>% bind_tf_idf(word, created_at, n)

top_tfidf<-tidy_trump_tfidf %>% arrange(desc(tf_idf))
top_tfidf$word[1]
```

# Sentiment Analysis
```{r}
head(get_sentiments("afinn"))

trump_tweet_sentiment <- tidy_trump_tweets %>% 
                          inner_join(get_sentiments("bing")) %>% 
                            count(created_at, sentiment) 
head(trump_tweet_sentiment)

# by Day
tidy_trump_tweets$date<-as.Date(tidy_trump_tweets$created_at)
trump_sentiment_plot <- tidy_trump_tweets %>% inner_join(get_sentiments("bing")) %>% 
      filter(sentiment=="negative") %>% count(date, sentiment)
ggplot(trump_sentiment_plot, aes(x=date, y=n))+
  geom_line(color="red")+
    theme_minimal()+
      ylab("Frequency of Negative Words in Trump's Tweets")+
        xlab("Date")

```


# Topic Modeling (LDA)
```{r}
library(topicmodels)
data("AssociatedPress") # news press dataset

AP_topic_model<-LDA(AssociatedPress, k=5, control = list(seed = 321)) # 5 is random
AP_topics <- tidy(AP_topic_model, matrix = "beta")

ap_top_terms <- 
  AP_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

# Structural Topic Modeling (individualized, help us validate the topic models)

```{r}
google_doc_id <- "1LcX-JnpGB0lU1iDnXnxB6WFqBywUKpew" # google file ID
poliblogs<-read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", google_doc_id), stringsAsFactors = FALSE)

library(stm)

# pre-processing most of the steps
processed <- textProcessor(poliblogs$documents, metadata = poliblogs)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta

First_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 10, prevalence =~ rating + s(day),
              max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)
plot(First_STM)

# find exmemplary passges
findThoughts(First_STM, texts = poliblogs$documents, n = 2, topics = 3)

# choose K - internal validity tests
findingk <- searchK(out$documents, out$vocab, K = c(10, 30), prevalence =~ rating + s(day), data = meta, verbose=FALSE) 
plot(findingk)

# Meta-data: for improving topic model classification
predict_topics<-estimateEffect(formula = 1:10 ~ rating + s(day), stmobj = First_STM, metadata = out$meta, uncertainty = "Global")

plot(predict_topics, covariate = "rating", topics = c(3, 5, 9),
 model = First_STM, method = "difference",
 cov.value1 = "Liberal", cov.value2 = "Conservative",
 xlab = "More Conservative ... More Liberal",
 main = "Effect of Liberal vs. Conservative",
 xlim = c(-.1, .1), labeltype = "custom",
 custom.labels = c('Topic 3', 'Topic 5','Topic 9'))

# Plotting topic relevance 
plot(predict_topics, "day", method = "continuous", topics = 3,
  model = z, printlegend = FALSE, xaxt = "n", xlab = "Time (2008)")
  monthseq <- seq(from = as.Date("2008-01-01"),
  to = as.Date("2008-12-01"), by = "month")
  monthnames <- months(monthseq)
  axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)),
  labels = monthnames)
```


# Text network ("textnets" package)
```{r}
library(devtools)
install_github("cbail/textnets") # needs R 3.5

library(textnets)
data(sotu)
sotu_first_speeches <- sotu %>% group_by(president) %>% slice(1L)

# this is a tidytext object 
prepped_sotu <- PrepText(sotu_first_speeches, groupvar = "president", textvar = "sotu_text", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
save(prepped_sotu, file = "prepped_sotu.Rdata")

# this is an iGraph object (disparity filters)
sotu_text_network <- CreateTextnet(prepped_sotu)
VisTextNet(sotu_text_network, label_degree_cut = 0)

# D3-based Viz
library(htmlwidgets)
vis <- VisTextNetD3(sotu_text_network, 
                      height=300,
                      width=400,
                      bound=FALSE,
                      zoom=FALSE,
                      charge=-30)
saveWidget(vis, "sotu_textnet.html")

# Tuning parameter Alpha 
VisTextNet(sotu_text_network, alpha=.1, label_degree_cut = 2)

# Community detection
sotu_communities <- TextCommunities(sotu_text_network)
head(sotu_communities)

top_words_modularity_classes <- InterpretText(sotu_text_network, prepped_sotu)
head(top_words_modularity_classes, 10)

# Centralities
text_centrality <- TextCentrality(sotu_text_network)


```



```{r}
library(dplyr)
library(ggplot2)
library(samplingbook)

survey<-read.csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day4-surveys/cleaned_mturk_jun18_data.csv", stringsAsFactors = FALSE)

demo <-read.csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day4-surveys/cleaned_acs16.csv", stringsAsFactors = FALSE)

pew <-read.csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2018/materials/day4-surveys/pew_Apr17_benchmarks.csv", stringsAsFactors = FALSE, header = FALSE)
colnames(pew) <- c("measure", "number")

survey <- survey[complete.cases(survey), ]
survey <- survey[survey$attention1 == "Donald Trump", ]

sample_avg <- apply(survey[,7:41], 2, mean, na.rm=TRUE)
sample_avg <- as.data.frame(sample_avg)
sample_avg$topic <- rownames(sample_avg)
sample_avg <- sample_avg %>% left_join(pew, by=c("topic" = "measure")) 

# Figure 1
ggplot(sample_avg, aes(x=number, y=sample_avg))+
  geom_point(color="blue")  + geom_smooth(method = "lm", se = FALSE, color="black", size=0.2)  +
    theme_bw()+
      ylab("MTurk Raw")+
        xlab("Pew") + ggtitle("Comparison between Raw MTurk & Pew Data")


# Weighted
demo$proportion <- demo$POP/sum(demo$POP)
survey <-  survey %>% left_join(demo, by=c("race", "sex", "age_cat", "region")) 

sample_weighted <- apply(survey[,7:41], 2, weighted.mean, survey$proportion, na.rm=TRUE)
sample_weighted <- as.data.frame(sample_weighted)
sample_weighted$topic <- rownames(sample_weighted)
sample_weighted <- sample_weighted %>% left_join(pew, by=c("topic" = "measure")) 

ggplot(sample_weighted, aes(x=number, y=sample_weighted))+
  geom_point(color="blue")  + geom_smooth(method = "lm", se = FALSE, color="black", size=0.2)  +
    theme_bw()+
      ylab("MTurk Weighted")+
        xlab("Pew") + ggtitle("Comparison between Weighted MTurk & Pew Data")

# Raw Absolute Difference
sample_avg$abs_diff <- abs(sample_avg$sample_avg - sample_avg$number)
hist(sample_avg$abs_diff, breaks=16)

# Weighted Absolute Difference
sample_weighted$abs_diff <- abs(sample_weighted$sample_weighted - sample_weighted$number)
hist(sample_weighted$abs_diff, breaks=16)



# Horvitz-Thompson Estimation
# sample_avg$hte <- htestimate(sample_avg$sample_avg, 674, PI= , method = "ht")

```

